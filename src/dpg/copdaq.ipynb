{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of DPG \n",
    "\n",
    "Implementing the COPDAQ algorithm. \n",
    "Reference paper: <a href=\"https://proceedings.mlr.press/v32/silver14.pdf\">link</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear function approximator\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int = 2, \n",
    "        out_features: int = 1,\n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.weights = torch.randn((in_features, out_features), requires_grad=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, action, value, policy_action, policy_grad):\n",
    "        q_value = (action - policy_action) * policy_grad.T @ self.weights + value\n",
    "        return q_value\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int = 2, \n",
    "        out_features: int = 1,\n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.weights = torch.randn((in_features, out_features), requires_grad=True)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        return torch.functional.F.tanh(obs @ self.weights)\n",
    "        \n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int = 2, \n",
    "        out_features: int = 1,\n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)    \n",
    "        \n",
    "        self.weights = torch.randn((in_features, out_features))\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        return obs @ self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class RollingAverage:\n",
    "    def __init__(self, window_size):\n",
    "        self.window = deque(maxlen=window_size)\n",
    "        self.averages = []\n",
    "\n",
    "    def update(self, value):\n",
    "        self.window.append(value)\n",
    "        self.averages.append(self.get_average)\n",
    "\n",
    "    @property\n",
    "    def get_average(self):\n",
    "        return sum(self.window) / len(self.window) if self.window else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class BasicExperienceReplay:\n",
    "    \n",
    "    def __init__(self, buffer_len=5000):\n",
    "        self.store = {\n",
    "            'states' : deque(maxlen=buffer_len),\n",
    "            'actions' : deque(maxlen=buffer_len),\n",
    "            'rewards' : deque(maxlen=buffer_len),\n",
    "            'next_states' : deque(maxlen=buffer_len),\n",
    "            'next_actions' : deque(maxlen=buffer_len),\n",
    "            'dones' : deque(maxlen=buffer_len)\n",
    "        }\n",
    "    \n",
    "    def update(\n",
    "        self, \n",
    "        state, \n",
    "        action, \n",
    "        reward, \n",
    "        next_state,\n",
    "        next_action, \n",
    "        done\n",
    "    ):\n",
    "        self.store['states'].append(state)\n",
    "        self.store['actions'].append(action)\n",
    "        self.store['rewards'].append(reward)\n",
    "        self.store['next_states'].append(next_state)\n",
    "        self.store['next_actions'].append(next_action)\n",
    "        self.store['dones'].append(done)\n",
    "    \n",
    "    def sample(self, buffer_size):\n",
    "        states = random.choices(self.store['states'], k=buffer_size)\n",
    "        actions = random.choices(self.store['actions'], k=buffer_size)\n",
    "        rewards = random.choices(self.store['rewards'], k=buffer_size)\n",
    "        next_states = random.choices(self.store['next_states'], k=buffer_size)\n",
    "        next_actions = random.choices(self.store['next_actions'], k=buffer_size)\n",
    "        dones = random.choices(self.store['dones'], k=buffer_size)\n",
    "        \n",
    "        return (\n",
    "            torch.as_tensor(np.array(states), dtype=torch.float32),\n",
    "            torch.as_tensor(np.array(actions), dtype=torch.float32),\n",
    "            torch.as_tensor(np.array(rewards), dtype=torch.float32),\n",
    "            torch.as_tensor(np.array(next_states), dtype=torch.float32),\n",
    "            torch.as_tensor(np.array(next_actions), dtype=torch.float32),\n",
    "            torch.as_tensor(np.array(dones), dtype=torch.bool)\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.store['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env: gym.Env, \n",
    "    actor: nn.Module, \n",
    "    critic: nn.Module, \n",
    "    baseline: nn.Module, \n",
    "    batch_size: int | bool = 16,\n",
    "    update_step: int = 4, \n",
    "    gamma: float = 0.99,  \n",
    "    timesteps: int = 1000,\n",
    "    lr_w: float = 0.01,\n",
    "    lr_theta: float = 0.001, \n",
    "    lr_v: float = 0.01\n",
    "):\n",
    "        \n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    metrics = RollingAverage(20)\n",
    "    replay = BasicExperienceReplay()\n",
    "    action = env.action_space.sample()\n",
    "    for step in range(1, timesteps):\n",
    "        obs_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        next_action = env.action_space.sample()\n",
    "        replay.update(obs, action, reward, obs_prime, next_action, terminated or truncated)\n",
    "        \n",
    "        obs = obs_prime\n",
    "        action = next_action \n",
    "        \n",
    "        if len(replay) > batch_size and step % update_step == 0:\n",
    "            batch_states, batch_actions, batch_rewards, batch_state_primes, batch_next_actions, batch_dones = replay.sample(batch_size)\n",
    "            print(batch_states.shape, batch_actions.shape, batch_rewards.shape, batch_state_primes.shape, batch_next_actions.shape )\n",
    "            actor_actions = actor(batch_states)\n",
    "            values = baseline(batch_states)\n",
    "            \n",
    "            actor.weights.grad = None\n",
    "            actor_actions.sum().backward()\n",
    "            policy_grad = actor.weights.grad\n",
    "            q_values = critic(batch_actions, values, actor_actions, policy_grad.detach())\n",
    "            \n",
    "            actor_actions_next = actor(batch_state_primes)\n",
    "            actor.weights.grad = None\n",
    "            actor_actions_next.sum().backward()\n",
    "            policy_grad_next = actor.weights.grad\n",
    "            with torch.no_grad():\n",
    "                values_next = baseline(batch_state_primes)\n",
    "                q_values_prime = critic(batch_next_actions, values_next, actor_actions_next, policy_grad_next.detach())\n",
    "            \n",
    "            # td error\n",
    "            td_error = batch_rewards + gamma * q_values_prime - q_values\n",
    "            \n",
    "            # update actor weights\n",
    "            actor.weights = actor.weights + lr_theta * policy_grad * (policy_grad.T @ critic.weights)\n",
    "            \n",
    "            # update critic and baseline weights (16, 1) @ (2, 1) \n",
    "            phi = ((batch_actions - actor_actions) @ policy_grad)\n",
    "            critic.weights = critic.weights.detach() + lr_w * (td_error * phi).sum()\n",
    "            baseline.weights = baseline.weights.detach() + lr_v * policy_grad\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            obs, _ = env.reset()\n",
    "            action = env.action_space.sample()\n",
    "            metrics.update(ep_reward)\n",
    "            print(f'Step: {step} | Avg Reward: {metrics.get_average}')\n",
    "         \n",
    "    return metrics   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2]) torch.Size([16, 1]) torch.Size([16]) torch.Size([16, 2]) torch.Size([16, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x1 and 2x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m critic = Critic()\n\u001b[32m      3\u001b[39m baseline = Baseline()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m metric_store = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env, actor, critic, baseline, batch_size, update_step, gamma, timesteps, lr_w, lr_theta, lr_v)\u001b[39m\n\u001b[32m     54\u001b[39m actor.weights = actor.weights + lr_theta * policy_grad * (policy_grad.T @ critic.weights)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# update critic and baseline weights (2, 1) @ (8, 2) \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m phi = (\u001b[43m(\u001b[49m\u001b[43mbatch_actions\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_actions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_grad\u001b[49m)\n\u001b[32m     58\u001b[39m critic.weights = critic.weights.detach() + lr_w * (td_error * phi).sum()\n\u001b[32m     59\u001b[39m baseline.weights = baseline.weights.detach() + lr_v * policy_grad\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (16x1 and 2x1)"
     ]
    }
   ],
   "source": [
    "actor = Actor()\n",
    "critic = Critic()\n",
    "baseline = Baseline()\n",
    "\n",
    "metric_store = train(env, actor, critic, baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
